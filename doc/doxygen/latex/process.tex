In process, we manage each application as a process which consists of one or more threads (for multi-\/threaded CPU or GPU applications).\hypertarget{process_process_core}{}\section{Core Partition}\label{process_process_core}
\hypertarget{process_process_queue}{}\section{Queues}\label{process_process_queue}
We maintain two separate queues for the simulation; thread queue and block queue. Thread queue has CPU traces and block queue has GPU traces. Especially, block queue consists of thread list. Each list in the block queue will have threads from the same thread block. \begin{DoxySeeAlso}{See also}
\hyperlink{classprocess__manager__c_a0fa9dc97ed1a88aadc63b0a82ef24896}{process\_\-manager\_\-c::m\_\-thread\_\-queue} 

\hyperlink{classprocess__manager__c_a07f5bef183ba3a7f15ed362c562c1137}{process\_\-manager\_\-c::m\_\-block\_\-queue}
\end{DoxySeeAlso}
\hypertarget{process_process_schedule}{}\section{Thread Scheduling}\label{process_process_schedule}
When a core becomes available, based on the core type, it pulls out a thread from the corresponding queue. Each core has maximum number of threads that can run concurrently.\hypertarget{process_process_storage}{}\section{Storage Management}\label{process_process_storage}
Since each thread requires its own data structure and GPU simulation will have huge number of threads, GPU simulation will require huge memory consumption. However, since there are limited number of threads at a certain time, we just need to maintain data structure for those active threads. In each thread/block queue, it will have dummy thread structures. When a thread is actually scheduled to the core, its data structure will be allocated.

\begin{Desc}
\item[\hyperlink{todo__todo000002}{Todo}]We need to carefully handle core allocation process. multiple x86s, multi-\/threaded.. \end{Desc}
